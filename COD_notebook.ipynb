{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPRESSED OBJECT DETECTION\n",
    "\n",
    "**Extension of pruning and quantization to the task of object detection**\n",
    "\n",
    "In this work, we extended pruning, a compression technique which discards unnecessary model connections, and weight sharing techniques for the task of object detection. With our approach we are able to compress a state-of-the-art object detection model by 30.0% without a loss in performance. We also show that our compressed model can be easily initialized with existing pre-trained weights, and thus is able to fully utilize published state-of-the-art model zoos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from datetime import date, time, datetime\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "\n",
    "import utils\n",
    "\n",
    "import detectron2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple, Union\n",
    "from fvcore.common.file_io import PathManager\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random seed\n",
    "Set the random seed for both torch and numpy in order to make the results repricable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Register the dataset into the format that Detectron2 will understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "dirname = './dataset/data/'\n",
    "\n",
    "CLASS_NAMES = ('giraffe',\n",
    "                'person',\n",
    "                'zebra',\n",
    "                'elephant',\n",
    "                'impala',\n",
    "                'monkey',\n",
    "                'lion',\n",
    "                'leopard',\n",
    "                'crocodile',\n",
    "                'buffalo',\n",
    "                'hyna',\n",
    "                'bird',\n",
    "                'gorilla')\n",
    "\n",
    "def load_voc_instances(dirname: str, split: str, class_names: Union[List[str], Tuple[str, ...]]):\n",
    "    \"\"\"\n",
    "    Load Pascal VOC detection annotations to Detectron2 format.\n",
    "        Args:\n",
    "            dirname: Contain \"annotations\", \"images\", \"train.txt\", \"valid.txt\"\n",
    "            split (str): one of \"train\", \"valid\"\n",
    "            class_names: list or tuple of class names\n",
    "    \"\"\"\n",
    "    \n",
    "    with PathManager.open(os.path.join(dirname, split + \".txt\")) as f:\n",
    "        fileids = np.loadtxt(f, dtype=np.str)\n",
    "\n",
    "    # Needs to read many small annotation files. Makes sense at local\n",
    "    annotation_dirname = PathManager.get_local_path(os.path.join(dirname, \"annotations/\"))\n",
    "    dicts = []\n",
    "    for fileid in fileids:\n",
    "        anno_file = os.path.join(annotation_dirname, fileid + \".xml\")\n",
    "        jpeg_file = os.path.join(dirname, \"images/\", fileid + \".jpg\")\n",
    "\n",
    "        with PathManager.open(anno_file) as f:\n",
    "            tree = ET.parse(f)\n",
    "\n",
    "        r = {\n",
    "            \"file_name\": jpeg_file,\n",
    "            \"image_id\": fileid,\n",
    "            \"height\": int(tree.findall(\"./size/height\")[0].text),\n",
    "            \"width\": int(tree.findall(\"./size/width\")[0].text),\n",
    "        }\n",
    "        instances = []\n",
    "\n",
    "        for obj in tree.findall(\"object\"):\n",
    "            cls = obj.find(\"name\").text\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            bbox = [float(bbox.find(x).text) for x in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
    "            bbox[0] -= 1.0\n",
    "            bbox[1] -= 1.0\n",
    "            instances.append(\n",
    "                {\"category_id\": class_names.index(cls), \"bbox\": bbox, \"bbox_mode\": BoxMode.XYXY_ABS}\n",
    "            )\n",
    "        r[\"annotations\"] = instances\n",
    "        dicts.append(r)\n",
    "    return dicts\n",
    "\n",
    "\n",
    "def register_pascal_voc(name, dirname, split, year, class_names=CLASS_NAMES):\n",
    "    meta_data = DatasetCatalog.register(name, lambda: load_voc_instances(dirname, split, class_names))\n",
    "    catalog = MetadataCatalog.get(name).set(\n",
    "        thing_classes=list(class_names), dirname=dirname, year=year, split=split\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "register_pascal_voc('train_data', dirname, split = 'train', year='2020', class_names=CLASS_NAMES)\n",
    "train_dataset_dicts = DatasetCatalog.get('train_data')\n",
    "train_metadata=MetadataCatalog.get('train_data')\n",
    "register_pascal_voc('valid_data', dirname, split = 'valid', year='2020', class_names=CLASS_NAMES)\n",
    "valid_dataset_dicts = DatasetCatalog.get('valid_data')\n",
    "valid_metadata=MetadataCatalog.get('valid_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataset_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at some to the training images\n",
    "\n",
    "images_index = [0,7,78,156,444]\n",
    "for index in images_index:\n",
    "    img = cv2.imread(train_dataset_dicts[index][\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(train_dataset_dicts[index])\n",
    "    im_path = train_dataset_dicts[index][\"file_name\"]\n",
    "    name = im_path.split('/')[-1]\n",
    "    my_im = vis.get_image()[:, :, ::-1]\n",
    "    filename = './temp_files/train_images/'+name\n",
    "    cv2.imwrite(filename, my_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "I will be using the faster_rcnn_R_50_FPN_3x with the coco initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "def faster_rcnn_R_50_FPN_3x(init_weights_flag = True, path='./chek_points'):\n",
    "\n",
    "\n",
    "      \"\"\"\n",
    "      This function takes two parameters:\n",
    "        1.  init_weights_flag : This is a flag that defines the initial weights of the backbone network of our R-CNN\n",
    "        2. path : this is the path where I need to save my model\n",
    "      \n",
    "      This function returns the predictor, after training the model it will loads the weights from the directory and the model configurations(cfg)\n",
    "\n",
    "      Parameters are defined in the same way as our lecturer said:\n",
    "        train for 800 iterations, a start learning rate of 0.02, 2 images per batch, and 128 regions per batch\n",
    "      \"\"\"\n",
    "      path = path + utils.model_dir()\n",
    "      utils.handle_dirs(path)\n",
    "\n",
    "      cfg = get_cfg()\n",
    "      cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "      cfg.DATASETS.TRAIN = (\"train_data\",)\n",
    "      cfg.DATASETS.TEST = ()\n",
    "      cfg.DATALOADER.NUM_WORKERS = 4\n",
    "      cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "      cfg.SOLVER.IMS_PER_BATCH = 8 # images per batch\n",
    "      cfg.SOLVER.BASE_LR = 0.02  # Learning rate\n",
    "      cfg.SOLVER.MAX_ITER = 2000    # number of iterations \n",
    "      cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 #regions per batch\n",
    "      cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13  #THE NUMBER OF classes\n",
    "      cfg.with_coco_init = init_weights_flag # Flag to initialize weight to COCO or IMAGENET\n",
    "\n",
    "      #create a path where I will store my model, this path defers for each model\n",
    "      os.makedirs(path, exist_ok=True)\n",
    "      cfg.OUTPUT_DIR = path\n",
    "      trainer = DefaultTrainer(cfg) \n",
    "      trainer.resume_or_load(resume=False)\n",
    "      trainer.train()\n",
    "\n",
    "      ## After training lets return the predictor\n",
    "      cfg.MODEL.WEIGHTS = os.path.join(path, \"model_final.pth\")\n",
    "      cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.67   # set the testing threshold for this model\n",
    "      cfg.DATASETS.TEST = (\"valid_data\", )\n",
    "\n",
    "      return DefaultPredictor(cfg),cfg,trainer\n",
    "#coco_init_predictor, coco_init_cfg, coco_init_trainer = faster_rcnn_R_50_FPN_3x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred, conff, treen = faster_rcnn_R_50_FPN_3x()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoints(path, filename):\n",
    "    \"\"\"\n",
    "    This function will load the checkpoints\n",
    "    @parameter: path==>Is the path to the directory where the check points are stored\n",
    "    @parameter: filename==> is the file name of your checkpoints, it is a .pth file\n",
    "    \n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.DATASETS.TRAIN = (\"train_data\",)\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = 4\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(path,filename)\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 8 # images per batch\n",
    "    cfg.SOLVER.BASE_LR = 0.02  # Learning rate\n",
    "    cfg.SOLVER.MAX_ITER = 10    # number of iterations \n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 #regions per batch\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13  #THE NUMBER OF classes\n",
    "    #create a path where I will store my model, this path defers for each model\n",
    "    cfg.OUTPUT_DIR = path\n",
    "    trainer = DefaultTrainer(cfg) \n",
    "    trainer.resume_or_load(resume=False)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.67   # set the testing threshold for this model\n",
    "    cfg.DATASETS.TEST = (\"valid_data\", )\n",
    "\n",
    "    return DefaultPredictor(cfg),cfg,trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"path where your checkpoint are stored\"\n",
    "CHECKPOINT = \"model_final.pth\"\n",
    "predictor, cfg_file, trainer_file = load_checkpoints(CHECKPOINT_DIR, CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ---------Final results--------------\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import pandas as pd\n",
    "import csv\n",
    "def model_evaluator(cfg,model,name='FULL_PRECISION',pruned=''):\n",
    "    save_dir = './RESULTS/'+ name + pruned\n",
    "    utils.handle_dirs(save_dir)\n",
    "    PATH = save_dir + '/checkpoints'\n",
    "    utils.handle_dirs(PATH)\n",
    "    PATH = PATH + '/model.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    evaluator = COCOEvaluator(\"valid_data\", cfg, False, output_dir=save_dir)\n",
    "    val_loader = build_detection_test_loader(cfg, \"valid_data\")\n",
    "    inference_on_dataset(model, val_loader, evaluator)\n",
    "    evaluations = inference_on_dataset(model,val_loader,evaluator)\n",
    "    t = evaluations['bbox']\n",
    "    with open(save_dir+'/evaluations.csv', 'w') as f:\n",
    "        for key in t.keys():\n",
    "            f.write(\"%s,%s\\n\"%(key,t[key]))\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_evaluator(cfg_file, trainer_file.model,name='L1',pruned='_100_percent_pruned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "# from detectron2.data import build_detection_test_loader\n",
    "# save_dir = './chek_points' + utils.model_dir('/evaluation')\n",
    "# utils.handle_dirs(save_dir)\n",
    "# evaluator = COCOEvaluator(\"valid_data\", coco_init_cfg, False, output_dir=save_dir)\n",
    "# val_loader = build_detection_test_loader(coco_init_cfg, \"valid_data\")\n",
    "# inference_on_dataset(coco_init_trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flops count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_metadata\n",
    "from detectron2.utils.analysis import flop_count_operators\n",
    "def mean_flops_counter(model,images_dict):\n",
    "    \"\"\"\n",
    "    This function count flops of a model, it returns the mean of flops over the given dataset.\n",
    "    @parameter: model==>this is the model you want to count flops\n",
    "    @parameter: image==>This is an input images in detectron2 format (list[dict])\n",
    "    \n",
    "    \"\"\"\n",
    "    average_flops = []\n",
    "    for image in images_dict:\n",
    "        image[\"image\"] =  torch.Tensor(cv2.imread(image[\"file_name\"])).permute(2,0,1)\n",
    "        flops_per_imange = flop_count_operators(model, [image])\n",
    "        average_flops.append(flops_per_imange['conv'])\n",
    "    \n",
    "    return sum(average_flops)/len(average_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainer_file.model\n",
    "# mean_flops_counter(model,valid_dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. MODEL PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer_file.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    \n",
    "#####res2\n",
    "(model.backbone.bottom_up.res2[0].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res2[0].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res2[0].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res2[1].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res2[1].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res2[1].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res2[2].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res2[2].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res2[2].conv3,  'weight'),\n",
    "    \n",
    "    \n",
    "#####res3\n",
    "(model.backbone.bottom_up.res3[0].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res3[0].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res3[0].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res3[1].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res3[1].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res3[1].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res3[2].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res3[2].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res3[2].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res3[3].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res3[3].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res3[3].conv3,  'weight'),\n",
    "\n",
    "#####res4\n",
    "(model.backbone.bottom_up.res4[0].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[0].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[0].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res4[1].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[1].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[1].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res4[2].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[2].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[2].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res4[3].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[3].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[3].conv3,  'weight'),\n",
    "    \n",
    "(model.backbone.bottom_up.res4[4].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[4].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[4].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res4[5].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res4[5].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res4[5].conv3,  'weight'),\n",
    "\n",
    "#####res5\n",
    "(model.backbone.bottom_up.res5[0].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res5[0].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res5[0].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res5[1].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res5[1].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res5[1].conv3,  'weight'),\n",
    "\n",
    "(model.backbone.bottom_up.res5[2].conv1,  'weight'),\n",
    "(model.backbone.bottom_up.res5[2].conv2,  'weight'),\n",
    "(model.backbone.bottom_up.res5[2].conv3,  'weight')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After removing the re-parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    \n",
    "    if i < 3:\n",
    "        #res2\n",
    "        prune.remove(model.backbone.bottom_up.res2[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res2[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res2[i].conv3,  'weight')\n",
    "\n",
    "        #res3\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv3,  'weight')\n",
    "\n",
    "        #res4\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv3,  'weight')\n",
    "\n",
    "        #res5\n",
    "        prune.remove(model.backbone.bottom_up.res5[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res5[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res5[i].conv3,  'weight')\n",
    "    \n",
    "    elif i < 4:\n",
    "        #res3\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res3[i].conv3,  'weight')\n",
    "        \n",
    "        #res4\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv3,  'weight')\n",
    "        \n",
    "    else:\n",
    "        #res4\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv1,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv2,  'weight')\n",
    "        prune.remove(model.backbone.bottom_up.res4[i].conv3,  'weight')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
